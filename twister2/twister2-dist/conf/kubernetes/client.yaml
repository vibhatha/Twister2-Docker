################################################################################
# Client configuration parameters for submission of twister2 jobs
################################################################################

# twister2 job name
# twister2.job.name: "reduce-stream-bench"
twister2.job.name: "t2-job"

# number of cores for each worker
# it is a floating point number
# each worker can have fractional cores such as 0.5 cores or multiple cores as 2
# default value is 1.0 core
twister2.worker.cpu: 0.5

# amount of memory for each worker in the job in mega bytes as integer
# default value is 200 MB
twister2.worker.ram: 1024

# amount of volatile hard disk space on each worker in giga bytes as a double
# set this value to zero if you will not use volatile hard disk space
# when its value is zero, no volatile disk will be setup of this worker
# default value is 0 GB. 
twister2.worker.volatile.disk: 1.0

# number of worker instances in this job
twister2.worker.instances: 6

########################################################################
# Kubernetes related settings
########################################################################
# namespace to use in kubernetes
# default value is "default"
# kubernetes.namespace: "default"

# number of workers (containers) per pod
# default is 1
# twister2.worker.instances must be divisible by containers_per_pod
kubernetes.workers.per.pod: 2

########################################################################
# Node locations related settings
########################################################################
# If this parameter is set as true,
# Twister2 will use the below lists for node locations:
#   kubernetes.datacenters.list
#   kubernetes.racks.list
# Otherwise, it will try to get these information by querying Kubernetes Master
# It will use below two labels when querying node locations
# For this to work, submitting client has to have admin privileges
kubernetes.node.locations.from.config: false

# rack label key for Kubernetes nodes in a cluster
# each rack should have a unique label
# all nodes in a rack should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: rack=rack1, rack=rack2, rack=rack3, etc
# no default value is specified
kubernetes.rack.labey.key: rack

# data center label key
# each data center should have a unique label
# all nodes in a data center should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: datacenter=dc1, datacenter=dc1, datacenter=dc1, etc
# no default value is specified
kubernetes.datacenter.labey.key: datacenter

# Data center list with rack names
kubernetes.datacenters.list:
- echo: ['blue-rack', 'green-rack']

# Rack list with node IPs in them
kubernetes.racks.list:
- blue-rack: ['node01.ip', 'node02.ip', 'node03.ip']
- green-rack: ['node11.ip', 'node12.ip', 'node13.ip']

########################################################################
# persistent volume related settings
########################################################################
# persistent volume size per worker in GB as double
# default value is 0.0Gi
# set this value to zero, if you have not persistent disk support
# when this value is zero, twister2 will not try to set up persistent storage for this job
persistent.volume.per.worker: 1.0

# the admin should provide a PersistentVolume object with the following storage class.
# Default storage class name is "twister2".
kubernetes.persistent.storage.class: "twister2-nfs-storage"

# persistent storage access mode. 
# It shows the access mode for workers to access the shared persistent storage.
# if it is "ReadWriteMany", many workers can read and write
# other alternatives: "ReadWriteOnce", "ReadOnlyMany"
# https://kubernetes.io/docs/concepts/storage/persistent-volumes
kubernetes.storage.access.mode: "ReadWriteMany"
